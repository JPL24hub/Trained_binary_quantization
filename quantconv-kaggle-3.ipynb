{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33149,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":27754},{"sourceId":35388,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":29777},{"sourceId":35987,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":30308}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n        \nfrom tqdm import tqdm\n\n!pip install torchviz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T15:45:13.882968Z","iopub.execute_input":"2024-04-22T15:45:13.883688Z","iopub.status.idle":"2024-04-22T15:45:42.063017Z","shell.execute_reply.started":"2024-04-22T15:45:13.883656Z","shell.execute_reply":"2024-04-22T15:45:42.061530Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchviz\n  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.1.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\nBuilding wheels for collected packages: torchviz\n  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=5c653786d9557f67b4b82eb399c45aa694ec80d512349524e06682be5271580a\n  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\nSuccessfully built torchviz\nInstalling collected packages: torchviz\nSuccessfully installed torchviz-0.0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:45:42.065619Z","iopub.execute_input":"2024-04-22T15:45:42.066217Z","iopub.status.idle":"2024-04-22T15:45:43.218622Z","shell.execute_reply.started":"2024-04-22T15:45:42.066184Z","shell.execute_reply":"2024-04-22T15:45:43.217615Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Mon Apr 22 15:45:43 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Wandb","metadata":{}},{"cell_type":"code","source":"!pip install wandb\n!wandb login # Rajouter la clÃ©","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:45:43.220342Z","iopub.execute_input":"2024-04-22T15:45:43.220779Z","iopub.status.idle":"2024-04-22T15:45:59.657129Z","shell.execute_reply.started":"2024-04-22T15:45:43.220737Z","shell.execute_reply":"2024-04-22T15:45:59.656136Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.5)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.44.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Custom model","metadata":{}},{"cell_type":"code","source":"class Conv2D_QuantFunction_Binary(torch.autograd.Function):\n    \n    @staticmethod\n    def forward(ctx, X, weight, Wp, Wn, threshold, stride, padding):\n            \n        # Max abs weight\n        max_abs_weight = torch.max(torch.abs(weight)).cuda()\n        normalized_weight = weight / max_abs_weight\n        normalized_weight = (weight / max_abs_weight).cuda()\n        \n        # binary weight \n        threshold = threshold.to(normalized_weight.device)\n        mask = torch.where(normalized_weight > threshold, 1, normalized_weight)\n        mask = torch.where(mask < threshold, -1, mask).cuda()\n        \n        quantized_weight = torch.where(mask == 1, Wp, mask)\n        quantized_weight = torch.where(mask == -1, Wn, mask).cuda()\n        \n        # sauvegarde des variables pour le backward\n        ctx.save_for_backward(X, \n                              quantized_weight,\n                              Wp.clone(),\n                              Wn.clone(),\n                              mask,\n                              stride,\n                              padding)\n        \n        return F.conv2d(X, quantized_weight, stride=stride.int().item(), padding=padding.int().item())\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        \n        # rÃ©cupÃ©ration des variables sauvegardÃ©es\n        X, quantized_weight, Wp, Wn, mask, stride, padding = ctx.saved_tensors\n        \n        # calcul gradient poids quantifiÃ©s et outpu\n        grad_input = torch.nn.grad.conv2d_input(X.shape, quantized_weight, grad_out, stride=stride, padding=padding)\n        grad_weight = torch.nn.grad.conv2d_weight(X, quantized_weight.shape, grad_out, stride=stride, padding=padding)\n        \n        # scale le gradient\n        grad_weight = torch.where(mask == 1, Wp * grad_weight, grad_weight)\n        grad_weight = torch.where(mask == -1, Wn * grad_weight, grad_weight) \n        \n        # calcul gradient Wp et Wn\n        grad_Wp = torch.sum(torch.where(mask == 1, grad_weight, torch.zeros_like(grad_weight))).unsqueeze(0)\n        grad_Wn = torch.sum(torch.where(mask == -1, grad_weight, torch.zeros_like(grad_weight))).unsqueeze(0)\n        \n        return grad_input, grad_weight, grad_Wp, grad_Wn, grad_Wn, None, None # None car pas de gradient pour le threshold\n\nclass Conv2D_QuantFunction_Ternary(torch.autograd.Function):\n    \n    @staticmethod\n    def forward(ctx, X, weight, Wp, Wn, threshold, stride, padding):\n        \n        # Max abs weight\n        max_abs_weight = torch.max(torch.abs(weight))\n        normalized_weight = (weight / max_abs_weight)\n        \n        # Ternary weight\n        threshold = threshold.to(normalized_weight.device)\n        mask = torch.where((normalized_weight > -threshold) & (normalized_weight <= threshold), 0, normalized_weight)\n        mask = torch.where(mask > threshold, 1, mask)\n        mask = torch.where(mask < -threshold, -1, mask)\n        \n        quantized_weight = torch.where(mask == 0, 0, mask)\n        quantized_weight = torch.where(mask == 1, Wp, mask)\n        quantized_weight = torch.where(mask == -1, Wn, mask)\n\n        # sauvegarde des variables pour le backward\n        ctx.save_for_backward(X, \n                              quantized_weight,\n                              Wp.clone(),\n                              Wn.clone(),\n                              mask,\n                              stride,\n                              padding)\n        return F.conv2d(X, quantized_weight, stride=stride.int().item(), padding=padding.int().item())\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        \n        # rÃ©cupÃ©ration des variables sauvegardÃ©es\n        X, quantized_weight, Wp, Wn, mask, stride, padding = ctx.saved_tensors\n        \n        # calcul gradient poids quantifiÃ©s et outpu\n        grad_input = torch.nn.grad.conv2d_input(X.shape, quantized_weight, grad_out, stride=stride, padding=padding)\n        grad_weight = torch.nn.grad.conv2d_weight(X, quantized_weight.shape, grad_out, stride=stride, padding=padding)\n        \n        # scale le gradient\n        grad_weight = torch.where(mask == 1, Wp * grad_weight, grad_weight)\n        grad_weight = torch.where(mask == -1, Wn * grad_weight, grad_weight) \n        \n        # calcul gradient Wp et Wn\n        grad_Wp = torch.sum(torch.where(mask == 1, grad_weight, torch.zeros_like(grad_weight))).unsqueeze(0)\n        grad_Wn = torch.sum(torch.where(mask == -1, grad_weight, torch.zeros_like(grad_weight))).unsqueeze(0)\n        \n        size_Wp = torch.sum(torch.where(mask == 1, 1, 0))\n        size_Wn = torch.sum(torch.where(mask == -1, 1, 0))\n        \n        grad_Wp = grad_Wp / size_Wp\n        grad_Wn = grad_Wn / size_Wn\n        \n        return grad_input, grad_weight, grad_Wp, grad_Wn, None, None, None # None car pas de gradient pour le threshold\n        \nclass Conv2D_QuantModule(nn.Module):\n    \n    def __init__(self, ternary, in_channels, out_channels, kernel_size, stride, padding, threshold=0.05, Wp_init=1, Wn_init=-1):\n        super().__init__()\n        # initialisation des poids et threshold\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        nn.init.xavier_normal_(self.weight)\n        \n        # Initialiser Wp avec la moyenne des poids positifs et Wn avec la moyenne des poids nÃ©gatifs\n        self.Wn = nn.Parameter(torch.tensor([Wn_init]).float())\n        self.Wp = nn.Parameter(torch.tensor([Wp_init]).float())\n        \n        # on doit manuellement passer tensor sur cuda, TODO rjaouter device dans Conv2D_QuantModule\n        self.threshold = torch.tensor([threshold], requires_grad=False).cuda()\n        self.stride = torch.tensor([stride]).float().cuda()\n        self.padding = torch.tensor([padding]).float().cuda()\n\n        self.function = Conv2D_QuantFunction_Ternary() if ternary else Conv2D_QuantFunction_Binary()\n            \n    def forward(self, X):\n        output =  self.function.apply(X, self.weight, self.Wp, self.Wn, self.threshold, self.stride, self.padding)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:45:59.660485Z","iopub.execute_input":"2024-04-22T15:45:59.661090Z","iopub.status.idle":"2024-04-22T15:45:59.691498Z","shell.execute_reply.started":"2024-04-22T15:45:59.661049Z","shell.execute_reply":"2024-04-22T15:45:59.690508Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Residual(nn.Module):\n    def __init__(self, ternary, quant, in_channels, out_channels, increase_dim=False, first=False):\n        super(Residual, self).__init__()\n        self.first = first\n        self.increase_dim = increase_dim\n\n        if increase_dim:\n            out_channels = in_channels * 2\n            stride1 = 2\n        else:\n            out_channels = in_channels\n            stride1 = 1\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = Conv2D_QuantModule(ternary, in_channels, out_channels, kernel_size=3, stride=stride1, padding=1) if quant else nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = Conv2D_QuantModule(ternary, out_channels, out_channels, kernel_size=3, stride=1, padding=1) if quant else nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if increase_dim:\n            self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n        self.in_channels = in_channels\n        \n    def forward(self, x):\n        if not self.first:\n            x = F.relu(self.bn1(x))\n        out = self.conv1(x)\n        out = F.relu(self.bn2(out))\n        out = self.conv2(out)\n\n        if self.increase_dim:\n            x = self.avgpool(x)\n            x = F.pad(x, (0, 0, 0, 0, self.in_channels // 2, self.in_channels // 2))\n\n        out += x\n        return out\n\nclass Model(nn.Module):\n    def __init__(self, n, quant, ternary):\n        super(Model, self).__init__()\n        self.n = n\n        \n        # jamais quantifier le premier conv \n        self.conv0 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn0 = nn.BatchNorm2d(16)\n        \n        self.res1 = self._make_layer(ternary, quant, 16, 16, n, first=True)\n        self.res2 = self._make_layer(ternary, quant, 16, 32, n, increase_dim=True)\n        self.res3 = self._make_layer(ternary, quant, 32, 64, n, increase_dim=True)\n        self.bnlast = nn.BatchNorm2d(64)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(64, 10)\n\n    def _make_layer(self, ternary, quant, input_channels, output_channels, num_blocks, increase_dim=False, first=False):\n        layers = []\n        layers.append(Residual(ternary, quant, input_channels, output_channels, increase_dim=increase_dim, first=first))\n        for _ in range(1, num_blocks):\n            layers.append(Residual(ternary, quant, output_channels, output_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv0(x)\n        x = F.relu(self.bn0(x))\n        \n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.res3(x)\n        x = F.relu(self.bnlast(x))\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:45:59.692900Z","iopub.execute_input":"2024-04-22T15:45:59.693321Z","iopub.status.idle":"2024-04-22T15:45:59.713845Z","shell.execute_reply.started":"2024-04-22T15:45:59.693250Z","shell.execute_reply":"2024-04-22T15:45:59.713160Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Fonction pour charger et prÃ©traiter les donnÃ©es CIFAR-10\ndef load_cifar10(batch_size):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  # Redimensionner les images pour ResNet-18\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalisation des images\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n\n    return trainloader, testloader\n\n# Fonction pour entraÃ®ner le modÃ¨le\ndef train(model, trainloader, testloader, criterion, optimizer, num_epochs, device='cuda', quant=False):\n    \n    device = next(model.parameters()).device\n    \n    # Learning rate decay \n    scheduler = StepLR(optimizer, step_size=20, gamma=0.1)  \n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        \n        model.train()\n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            if i % 100 == 99:    # Afficher la perte tous les 100 mini-lots\n                print('[%d, %5d] loss: %.3f' %\n                      (epoch + 1, i + 1, running_loss / 100))\n                wandb.log({\"Training Loss\": running_loss / 100})  # Enregistrer la perte\n                running_loss = 0.0\n                \n                if quant:\n                    #wandb.log({\"Wn\": model.module.layer1.layer[0].conv1.Wn.item(), \"Wp\": model.module.layer1.layer[0].conv1.Wp.item()})\n                    wandb.log({\"l1.0.conv1.Wn\": model.module.res1[0].conv1.Wn.item(), \"l1.0.conv1.Wp\": model.module.res1[0].conv1.Wp.item()})\n                    wandb.log({\"l3.2.conv2.Wn\": model.module.res3[1].conv2.Wn.item(), \"l3.2.conv2.Wp\": model.module.res3[1].conv2.Wp.item()})\n        \n        model.eval()\n        train_accuracy = evaluate_accuracy(model, trainloader)\n        test_accuracy = evaluate_accuracy(model, testloader)\n        wandb.log({\"Train accuracy\": train_accuracy, \"Test accuracy\": test_accuracy})\n        \n        scheduler.step()  \n        \n    print('Finished Training')\n\n# Fonction pour calculer l'accuracy\ndef evaluate_accuracy(model, testloader):\n    # Get the device where the model resides\n    device = next(model.parameters()).device\n\n    # Iterate through the test data and perform inference\n    total_correct = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            # Move data to the same device as the model\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Perform inference\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n\n            # Update counts\n            total_samples += labels.size(0)\n            total_correct += (predicted == labels).sum().item()\n\n    # Calculate accuracy\n    accuracy = total_correct / total_samples\n\n    print('Accuracy on the test set: {:.2f}%'.format(accuracy * 100))\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:45:59.715289Z","iopub.execute_input":"2024-04-22T15:45:59.715837Z","iopub.status.idle":"2024-04-22T15:45:59.735038Z","shell.execute_reply.started":"2024-04-22T15:45:59.715798Z","shell.execute_reply":"2024-04-22T15:45:59.733960Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom torch.optim.lr_scheduler import StepLR\n\ntorch.cuda.empty_cache()\n\nconfig = {\n    \"batch_size\": 128,\n    \"num_epochs\": 20,\n    \"learning_rate\": 0.1,\n    \"momentum\": 0.9,\n    \"n\": 5,  ,             # 3 -> Resnet20, 5 -> Resnet32, 7 -> Resnet42\n    \"quant\": True,         # False if we need to train without quantization\n    \"ternary\": True        # only if config.quant = True, True for ternary quantization, False for binary quantization\n}\n\nrun = wandb.init(\n    # Set the project where this run will be logged\n    project=\"quantization-aware-training\",\n    group=\"quant-Resnet-training\",\n    config=config\n)\n\n# Charger les donnÃ©es CIFAR-10\ntrainloader, testloader = load_cifar10(batch_size=config[\"batch_size\"])\n\n# Charger le modÃ¨le ResNet-18 \nmodel = Model(n=config[\"n\"], quant=config[\"quant\"], ternary=config[\"ternary\"])\n\nif config[\"quant\"]:\n    weights = torch.load('/kaggle/input/resnet32/pytorch/acc80/1/resnet_32_accuracy_80.pth', map_location='cuda')\n\n    # ajouter les Wn, Wp\n    for k,v in model.state_dict().items():\n        if \"Wn\" in k or \"Wp\" in k:\n            weights[k] = v\n\n    # Load the weights into the model\n    model.load_state_dict(weights)\n\n# Activer la surveillance WandB pour le modÃ¨le\nwandb.watch(model, log='all', log_freq=10)\n\n# DÃ©finir la fonction de perte et l'optimiseur\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=0.0002)\n\n# Mettre sur plusieurs GPU\nif torch.cuda.device_count() == 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model.cuda()\nelif torch.cuda.device_count() == 2:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model.to('cuda:0')\n    model = nn.DataParallel(model)  # Utiliser DataParallel pour utiliser plusieurs GPU\nelse:\n    print(\"Using cpu\")\n    \n# EntraÃ®ner le modÃ¨le\nmodel.eval()\ntrain_accuracy = evaluate_accuracy(model, trainloader)\ntest_accuracy = evaluate_accuracy(model, testloader)\nwandb.log({\"Train accuracy\": train_accuracy, \"Test accuracy\": test_accuracy})\n\ntrain(model, trainloader, testloader, criterion, optimizer, num_epochs=config[\"num_epochs\"], quant=config[\"quant\"])\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:45:59.736348Z","iopub.execute_input":"2024-04-22T15:45:59.736621Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33md-zhu66\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240422_154602-gufnd4qo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/d-zhu66/quantization-aware-training/runs/gufnd4qo/workspace' target=\"_blank\">solar-smoke-218</a></strong> to <a href='https://wandb.ai/d-zhu66/quantization-aware-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/d-zhu66/quantization-aware-training' target=\"_blank\">https://wandb.ai/d-zhu66/quantization-aware-training</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/d-zhu66/quantization-aware-training/runs/gufnd4qo/workspace' target=\"_blank\">https://wandb.ai/d-zhu66/quantization-aware-training/runs/gufnd4qo/workspace</a>"},"metadata":{}},{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|ââââââââââ| 170498071/170498071 [00:02<00:00, 71761632.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nUsing 2 GPUs!\nAccuracy on the test set: 10.00%\nAccuracy on the test set: 10.00%\n[1,   100] loss: 2.137\n[1,   200] loss: 1.659\n[1,   300] loss: 1.472\nAccuracy on the test set: 36.81%\nAccuracy on the test set: 36.56%\n[2,   100] loss: 1.267\n[2,   200] loss: 1.201\n[2,   300] loss: 1.135\nAccuracy on the test set: 59.28%\nAccuracy on the test set: 58.24%\n[3,   100] loss: 0.986\n[3,   200] loss: 0.965\n[3,   300] loss: 0.942\nAccuracy on the test set: 62.51%\nAccuracy on the test set: 61.39%\n[4,   100] loss: 0.877\n[4,   200] loss: 0.882\n[4,   300] loss: 0.866\nAccuracy on the test set: 53.87%\nAccuracy on the test set: 52.88%\n[5,   100] loss: 0.828\n[5,   200] loss: 0.826\n[5,   300] loss: 0.812\nAccuracy on the test set: 64.72%\nAccuracy on the test set: 63.52%\n","output_type":"stream"}]},{"cell_type":"code","source":"train_accuracy = evaluate_accuracy(quant_resnet18, trainloader)\ntest_accuracy = evaluate_accuracy(quant_resnet18, testloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(quant_resnet18.state_dict(), '/kaggle/working/resnet_32_accuracy_80.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}